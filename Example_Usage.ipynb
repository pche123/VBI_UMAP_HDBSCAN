{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d79bf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 2. Have a re-think / re-check wrt data leakage\n",
    "# 3. Comment on the functions in the module. Add more explanations here. Finish the README write up including library version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100182f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db0cf0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import methodology, processing, utils\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mpl.rcParams['figure.figsize']=[15, 10]\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "mpl.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "mpl.rcParams['font.size'] = '32'   # 32 before # 22 for the bad case\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064276e",
   "metadata": {},
   "source": [
    "# Parameter Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e43940",
   "metadata": {},
   "source": [
    "## Parameters for reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a9280d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFFT = 2**7\n",
    "DATA_ROOT_LOC = '../../../data/'             # Where data is located relative to current notebook file\n",
    "NUM_DATA_IN_DIR = 400\n",
    "\n",
    "Bridge_Type = 'B09'                          # B09 | B15 | B21 | B27 | B33 | B39\n",
    "Damage_Location = 'DL25'                     # DL25 | DL50\n",
    "Damage_Levels = ['DM00', 'DM20', 'DM40']     # DM00 (\"Healthy\") | DM20 | DM40\n",
    "Vehicle_Type = 'V1'                          # V1 | V2 | V5\n",
    "Bridge_Profile = 'P00'                       # P00 | PA1 | PA2\n",
    "\n",
    "DOF_NUM = 2 # Vehicle V1 has 2 DOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2b134",
   "metadata": {},
   "source": [
    "## (Hyper)Parameters for data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82451c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "n_neighbors = 60 # 30 is good\n",
    "min_dist = 0\n",
    "set_op_mix_ratio = 0.90\n",
    "total_obs = 2\n",
    "min_cluster_size = 95\n",
    "sample_times = 500 # 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d5c54",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e253e4a",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb4447",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc099f5",
   "metadata": {},
   "source": [
    "# Preprocessing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3a261",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8287864",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/B09/DL25/DM00/V1/P00/B09DL25DM00V1P00E0001.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/B09/DL25/DM00/V1/P00/B09DL25DM00V1P00E0001.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_data_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBridge_Type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDamage_Location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVehicle_Type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBridge_Profile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mNUM_DATA_IN_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_DATA_IN_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mDATA_ROOT_LOC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDATA_ROOT_LOC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mNFFT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNFFT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Mehri_International_Collab/github/VBI_UMAP_HDBSCAN/src/utils.py:79\u001b[0m, in \u001b[0;36mread_data_fft\u001b[0;34m(Bridge_Type, Damage_Location, Vehicle_Type, Bridge_Profile, Damage_Levels, NUM_DATA_IN_DIR, DATA_ROOT_LOC, NFFT)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,NUM_DATA_IN_DIR\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m): \n\u001b[1;32m     78\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m DATA_ROOT_LOC \u001b[38;5;241m+\u001b[39m str_root_location \u001b[38;5;241m+\u001b[39m str_root_event_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 79\u001b[0m     data_read \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     start_pos \u001b[38;5;241m=\u001b[39m data_read[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVeh\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt0_ind_beam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     82\u001b[0m     end_pos \u001b[38;5;241m=\u001b[39m data_read[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVeh\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_end_ind_beam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Mehri_International_Collab/github/VBI_UMAP_HDBSCAN/src/utils.py:38\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadmat\u001b[39m(filename):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    this function should be called instead of direct scipy.io .loadmat\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    as it cures the problem of not properly recovering python dictionaries\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    from mat files. It calls the function check keys to cure all entries\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    which are still mat-objects\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct_as_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze_me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check_keys(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:224\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    225\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/B09/DL25/DM00/V1/P00/B09DL25DM00V1P00E0001.mat'"
     ]
    }
   ],
   "source": [
    "data = utils.read_data_fft(Bridge_Type, Damage_Location, Vehicle_Type, Bridge_Profile,\n",
    "                    NUM_DATA_IN_DIR = NUM_DATA_IN_DIR , \n",
    "                    DATA_ROOT_LOC = DATA_ROOT_LOC, \n",
    "                    NFFT=NFFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_flag_data = True\n",
    "for dof in range(DOF_NUM):\n",
    "    \n",
    "    if plt_flag_data:\n",
    "        plt.figure()\n",
    "        # If you intend to plot other parts further on\n",
    "        # Then please ensure the following code *is* run. \n",
    "        # It helps with the labelling of the plot in a loop.\n",
    "        #######################\n",
    "        label_healthy = ['DM00'] \n",
    "        label_moderate = ['DM20']\n",
    "        label_large = ['DM40']\n",
    "        for i in range(1,50):\n",
    "            label_healthy += [None]\n",
    "            label_moderate += [None]\n",
    "            label_large += [None]\n",
    "        #########################\n",
    "\n",
    "        # FFT only\n",
    "        plt.title(f'DOF NUMBER: {dof+1}')\n",
    "        # We take columns 1 --> end, because there is a 'zero' occuring at index 0\n",
    "        plt.plot(abs(data[dof]['DM00'][0:50,1:].T),'g', alpha = 0.4, label = label_healthy);\n",
    "        plt.plot(abs(data[dof]['DM20'][0:50,1:].T),'b', alpha = 0.4, label = label_moderate);\n",
    "        plt.plot(abs(data[dof]['DM40'][0:50,1:].T),'r', alpha = 0.4, label = label_large);\n",
    "        cols = abs(data[dof]['DM00'][:,1:].T).shape[1]\n",
    "\n",
    "\n",
    "        plt.xlim([0,60]) # Cut Off Frequency\n",
    "        plt.xlabel('Frequency [Hz]')\n",
    "        plt.ylabel('Magnitude')\n",
    "        leg = plt.legend()\n",
    "        for lh in leg.legendHandles: \n",
    "            lh.set_alpha(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18ccf4",
   "metadata": {},
   "source": [
    "## Analyzing the effect of bootstraping \n",
    "\n",
    "It is assumed here that we know which data is DM00, DM20, DM40. This is for demonstration purposes, in the actual code, it won't be known which data is which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOF_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_flag_data = True\n",
    "\n",
    "transformed = []\n",
    "for dof in range(DOF_NUM):\n",
    "    \n",
    "    transformed.append({})\n",
    "    if plt_flag_data:\n",
    "        plt.figure()\n",
    "        healthy_data_fft = data[dof]['DM00']\n",
    "        mod_data_fft = data[dof]['DM20']\n",
    "        large_data_fft = data[dof]['DM40']\n",
    "\n",
    "        # Perform bootstrapping\n",
    "        healthy_bootstrap = processing.bootstrap_sampling(healthy_data_fft, sample_times=sample_times) \n",
    "        mod_bootstrap = processing.bootstrap_sampling(mod_data_fft, sample_times=sample_times) \n",
    "        large_bootstrap = processing.bootstrap_sampling(large_data_fft, sample_times=sample_times) \n",
    "\n",
    "        healthy_log_abs_train = processing.log_abs_transform(healthy_bootstrap[0])\n",
    "        mod_log_abs_train = processing.log_abs_transform(mod_bootstrap[0])\n",
    "        large_log_abs_train = processing.log_abs_transform(large_bootstrap[0])\n",
    "\n",
    "        plt.title(f'DOF NUMBER: {dof+1}')\n",
    "        plt.plot(healthy_log_abs_train[0:50,:].T,'g', label = label_healthy, alpha = 0.4);\n",
    "        plt.plot(mod_log_abs_train[0:50,:].T,'b', label = label_moderate, alpha = 0.4);\n",
    "        plt.plot(large_log_abs_train[0:50,:].T,'r', label = label_large, alpha = 0.4);\n",
    "        plt.xlim([0,60])\n",
    "        plt.xlabel('Frequency [Hz]')\n",
    "        plt.ylabel('Magnitude')\n",
    "\n",
    "        leg = plt.legend()\n",
    "        for lh in leg.legendHandles: \n",
    "            lh.set_alpha(1)\n",
    "            \n",
    "        transformed[dof]['DM00'] = healthy_log_abs_train\n",
    "        transformed[dof]['DM20'] = mod_log_abs_train\n",
    "        transformed[dof]['DM40'] = large_log_abs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228d284",
   "metadata": {},
   "source": [
    "## Demonstration of Standard Scaling effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_flag_data = True\n",
    "scaled = []\n",
    "\n",
    "for dof in range(DOF_NUM):\n",
    "    \n",
    "    scaled.append({})\n",
    "    if plt_flag_data:\n",
    "        plt.figure()\n",
    "        scaler = StandardScaler()\n",
    "        healthy_scaled = scaler.fit_transform(transformed[dof]['DM00']) #### You NEED TO MAKE SURE THIS SPLIT IS CONSISTENT E.G. WHEN INPUTING THE DATASTREAMS SEQUENCING\n",
    "        mod_scaled = scaler.transform(transformed[dof]['DM20'])\n",
    "        large_scaled = scaler.transform(transformed[dof]['DM40'])\n",
    "        \n",
    "        plt.title(f'DOF NUMBER: {dof+1}')\n",
    "        plt.plot(healthy_scaled[0:50,:].T,'g', label = label_healthy, alpha = 0.4);\n",
    "        plt.plot(mod_scaled[0:50,:].T,'b', label = label_moderate, alpha = 0.4);\n",
    "        plt.plot(large_scaled[0:50,:].T,'r', label = label_large, alpha = 0.4);\n",
    "\n",
    "        plt.xlim([0,60])\n",
    "        plt.xlabel('Frequency [Hz]')\n",
    "        plt.ylabel('Magnitude')\n",
    "\n",
    "        leg = plt.legend()\n",
    "        for lh in leg.legendHandles: \n",
    "            lh.set_alpha(1)\n",
    "            \n",
    "        scaled[dof]['DM00'] = healthy_scaled\n",
    "        scaled[dof]['DM20'] = mod_scaled\n",
    "        scaled[dof]['DM40'] = large_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1eb8a",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48a3e4",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd677ec",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4dceb6",
   "metadata": {},
   "source": [
    "# Running UMAP $\\rightarrow$ HDBSCAN Algorithm\n",
    "\n",
    "This is to be run on the *transformed* data as per above. In this code it therefore assumes that all plots are run (because the data is pre-processing in each instance).\n",
    "\n",
    "Note also that this function takes n_healty, n_mod, and n_large as inputs. It must be stressed that *these are not used* in the algorithm -- they are simply there to assist in the colors when plotting. The only input ever being using is that of the *total stacked data*.\n",
    "\n",
    "Looking at the previous scaling, the ```fit-transform``` was perform directly on the healthy, and the ```transform``` on the modeerate (DM20), and then large (DM40) cases. This may initially come across as a data leakage problem, but in fact this is not the case. Because we operate under the assumption that the first X amount of days / weeks / months, the bridge should be operating within a healthy state, and therefore it is OK to perform a fit-transform here. From there-onwards, it is appropriate to continually apply the t```ransform``` only regardless of the health state (because if it is still healthy, the data will map to the same space, and if it contains damage, it will map to a different location). \n",
    "\n",
    "This option can be explored through a hyperparameter in the ```HDBSCAN_UMAP_Application``` function via ```total_obs```. This hyperparameter works to partition the stacked data into distinct observation sets e.g. by default ```total_obs=2```, so we assume that the total data stack is split into two, so the first half will contain DM00 and DM20 data, whilst the second half will contain a mixture of DM20, and DM40 data. \n",
    "\n",
    "In a practical implementation, the scalar transformations *should* be more ideally performed within the ```HDBSCAN_UMAP_Application``` function (transform the data live -- as it is being received) but for the sake of demonstration (i.e. clearly separating out the steps), this has been externalized from the function in the above step.\n",
    "\n",
    "Note that even though there are 400 data in directory, beacuse we are over sampling (using the boot-strap like procedure) the ```n_healthy```, ```n_mod```, ```n_large``` will be the same as the ```sample_times``` chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca465d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_healthy, n_mod, n_large = sample_times, sample_times, sample_times\n",
    "for dof in range(DOF_NUM):\n",
    "    total_data = np.vstack((scaled[dof]['DM00'], scaled[dof]['DM20'], scaled[dof]['DM40']))\n",
    "    methodology.HDBSCAN_UMAP_Application(total_data, n_healthy, n_mod, n_large, dof, plot_flag = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4f770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "841282aa",
   "metadata": {},
   "source": [
    "Note that it is also possible to check the results obtained by the proposed procedure against theoretical results. That is, by performing the UMAP projection *only*, and then assigning a color to each row, with the knowledge that *we know* which cluster each row belongs to. This can be achieved using the following code. \n",
    "\n",
    "This time instead however, we shall set the ```total_obs``` value to 5. In other words, our stack of data arrives to us in 5 discrete increments / time observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918c68d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dof in range(DOF_NUM):\n",
    "    methodology.theoretical_clustering(total_data, n_healthy, n_mod, n_large, dof, plot_flag = True, total_obs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89022f85",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45514637",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafce55e",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd7831",
   "metadata": {},
   "source": [
    "# Conclusion / Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812d814",
   "metadata": {},
   "source": [
    "As can be seen above for the provided case of P00 - V01 - B09 - DL50, the data pipeline successfully spearates out each of the damage cases quite easily. The *exact* same hyper-parameters have been also shown to work on the following data cases:\n",
    "\n",
    " - P00 - V01 - B09 - DL25\n",
    " - PA1 - V01 - B09 - DL50\n",
    " - PA1 - V01 - B09 - DL25\n",
    " - PA2 - V01 - B09 - DL50\n",
    " \n",
    "There is however a slight difficulty when it comes to the case of PA2 - V01 - B09 - DL25 (DOF1), and hyper-parameter tuning will be required on behalf of the user. A quick solution (based on observation) is to \n",
    "\n",
    " 1. Increase ```n_neighbors``` to a value of 60 so as to increase the *diameter* the UMAP scans for clusters, \n",
    " 2. Increase ```min_cluster_size``` to a value of 95 for similar reasons, \n",
    " 3. Decrease ```set_op_mix_ratio``` to a value of 0.90, to slightly enforce further separation between any clusters found in the dimensionality reduction, and to \n",
    " 4. Increase ```sample_times``` to a value of 3000, to try and allow for the clusters to increase in overall density. \n",
    "\n",
    "It will be noted that even then, there is some \"overlap\" over data points so that they can be slightly mis-specified (i.e. the probability of allocation between clusters approaches an even split, so in the event of two clusters this would be a 50%). Any true mis-allocations of points can be shown easily via ```theoretical_clustering``` function. However these points will be *greyed-out* in the ```HDBSCAN_UMAP_Application``` function (moreover outlier points as determined from HDBSCAN will also be labelled). If one wants these not to be shown then one can set the ```remove_outliers``` and ```remove_misspec``` hyperparameters to ```True```. The method to determine these points is currently not optimal and based on determining the *mode* of a group labels (which are known ahead of time), therefore in practical settings, it is advised to be wary of applying this procedure to any other arbitrary dataset. \n",
    "\n",
    "Further, the projections for V01 - B09 were done to two dimensional space, for visual purposes. If the problem increases in complexity, then it may be necessary to increase ```n_components``` to a value greater than two.\n",
    "\n",
    "We hope this work inspires further development on such a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef16cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
