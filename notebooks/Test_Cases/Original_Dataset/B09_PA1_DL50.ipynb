{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79bf9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 2. Have a re-think / re-check wrt data leakage\n",
    "# 3. Comment on the functions in the module. Add more explanations here. Finish the README write up including library version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100182f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0cf0f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Adds higher directory to python modules path\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m methodology, processing, utils\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path\n",
    "\n",
    "from src import methodology, processing, utils\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mpl.rcParams['figure.figsize']=[15, 10]\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "mpl.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "mpl.rcParams['font.size'] = '32'   # Use 22 if legend overlaps on data\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064276e",
   "metadata": {},
   "source": [
    "# Parameter Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e43940",
   "metadata": {},
   "source": [
    "## Parameters for reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9280d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFFT = 2**9\n",
    "DATA_ROOT_LOC = '../../../data/'            # Where data is located relative to current notebook file\n",
    "NUM_DATA_IN_DIR = 400\n",
    "\n",
    "Bridge_Type = 'B09'                          # B09 | B15 | B21 | B27 | B33 | B39\n",
    "Damage_Location = 'DL50'                     # DL25 | DL50\n",
    "Damage_Levels = ['DM00', 'DM20', 'DM40']     # DM00 (\"Healthy\") | DM20 | DM40\n",
    "Vehicle_Type = 'V1'                          # V1 | V2 | V5\n",
    "Bridge_Profile = 'PA1'                       # P00 | PA1 | PA2\n",
    "\n",
    "DOF_NUM = 2 # Vehicle V1 has 2 DOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2b134",
   "metadata": {},
   "source": [
    "## (Hyper)Parameters for data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82451c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "n_neighbors = 60 # 30 is good\n",
    "min_dist = 0\n",
    "set_op_mix_ratio = 0.90\n",
    "total_obs = 2\n",
    "min_cluster_size = 95\n",
    "sample_times = 1000 # 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d5c54",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e253e4a",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb4447",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc099f5",
   "metadata": {},
   "source": [
    "# Preprocessing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3a261",
   "metadata": {},
   "source": [
    "## Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8287864",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.read_data_fft(Bridge_Type, Damage_Location, Vehicle_Type, Bridge_Profile,\n",
    "                    NUM_DATA_IN_DIR = NUM_DATA_IN_DIR , \n",
    "                    DATA_ROOT_LOC = DATA_ROOT_LOC, \n",
    "                    NFFT=NFFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_flag_data = True\n",
    "for dof in range(DOF_NUM):\n",
    "    \n",
    "    if plt_flag_data:\n",
    "        plt.figure()\n",
    "        # If you intend to plot other parts further on\n",
    "        # Then please ensure the following code *is* run. \n",
    "        # It helps with the labelling of the plot in a loop.\n",
    "        #######################\n",
    "        label_healthy = ['DM00'] \n",
    "        label_moderate = ['DM20']\n",
    "        label_large = ['DM40']\n",
    "        for i in range(1,50):\n",
    "            label_healthy += [None]\n",
    "            label_moderate += [None]\n",
    "            label_large += [None]\n",
    "        #########################\n",
    "\n",
    "        # FFT only\n",
    "        plt.title(f'DOF NUMBER: {dof+1}')\n",
    "        # We take columns 1 --> end, because there is a 'zero' occuring at index 0\n",
    "        plt.plot(abs(data[dof]['DM00'][0:50,1:].T),'g', alpha = 0.4, label = label_healthy);\n",
    "        plt.plot(abs(data[dof]['DM20'][0:50,1:].T),'b', alpha = 0.4, label = label_moderate);\n",
    "        plt.plot(abs(data[dof]['DM40'][0:50,1:].T),'r', alpha = 0.4, label = label_large);\n",
    "        cols = abs(data[dof]['DM00'][:,1:].T).shape[1]\n",
    "\n",
    "\n",
    "        plt.xlim([0,60]) # Cut Off Frequency\n",
    "        plt.xlabel('Frequency [Hz]')\n",
    "        plt.ylabel('Magnitude')\n",
    "        leg = plt.legend()\n",
    "        for lh in leg.legendHandles: \n",
    "            lh.set_alpha(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18ccf4",
   "metadata": {},
   "source": [
    "## Analyzing the effect of bootstraping \n",
    "\n",
    "It is assumed here that we know which data is DM00, DM20, DM40. This is for demonstration purposes, in the actual code, it won't be known which data is which"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff5db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOF_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_flag_data = True\n",
    "\n",
    "transformed = []\n",
    "for dof in range(DOF_NUM):\n",
    "    \n",
    "    transformed.append({})\n",
    "    if plt_flag_data:\n",
    "        plt.figure()\n",
    "        healthy_data_fft = data[dof]['DM00']\n",
    "        mod_data_fft = data[dof]['DM20']\n",
    "        large_data_fft = data[dof]['DM40']\n",
    "\n",
    "        # Perform bootstrapping\n",
    "        healthy_bootstrap = processing.bootstrap_sampling(healthy_data_fft, sample_times=sample_times) \n",
    "        mod_bootstrap = processing.bootstrap_sampling(mod_data_fft, sample_times=sample_times) \n",
    "        large_bootstrap = processing.bootstrap_sampling(large_data_fft, sample_times=sample_times) \n",
    "\n",
    "        healthy_log_abs_train = processing.log_abs_transform(healthy_bootstrap[0])\n",
    "        mod_log_abs_train = processing.log_abs_transform(mod_bootstrap[0])\n",
    "        large_log_abs_train = processing.log_abs_transform(large_bootstrap[0])\n",
    "\n",
    "        plt.title(f'DOF NUMBER: {dof+1}')\n",
    "        plt.plot(healthy_log_abs_train[0:50,3:].T,'g', label = label_healthy, alpha = 0.4);\n",
    "        plt.plot(mod_log_abs_train[0:50,3:].T,'b', label = label_moderate, alpha = 0.4);\n",
    "        plt.plot(large_log_abs_train[0:50,3:].T,'r', label = label_large, alpha = 0.4);\n",
    "        plt.xlim([0,60])\n",
    "        plt.xlabel('Frequency [Hz]')\n",
    "        plt.ylabel('Magnitude')\n",
    "\n",
    "        leg = plt.legend()\n",
    "        for lh in leg.legendHandles: \n",
    "            lh.set_alpha(1)\n",
    "            \n",
    "        transformed[dof]['DM00'] = healthy_log_abs_train[:,3:]\n",
    "        transformed[dof]['DM20'] = mod_log_abs_train[:,3:]\n",
    "        transformed[dof]['DM40'] = large_log_abs_train[:,3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228d284",
   "metadata": {},
   "source": [
    "## Demonstration of Standard Scaling effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_flag_data = True\n",
    "scaled = []\n",
    "\n",
    "for dof in range(DOF_NUM):\n",
    "    \n",
    "    scaled.append({})\n",
    "    if plt_flag_data:\n",
    "        plt.figure()\n",
    "        scaler = StandardScaler()\n",
    "        healthy_scaled = scaler.fit_transform(transformed[dof]['DM00']) #### You NEED TO MAKE SURE THIS SPLIT IS CONSISTENT E.G. WHEN INPUTING THE DATASTREAMS SEQUENCING\n",
    "        mod_scaled = scaler.transform(transformed[dof]['DM20'])\n",
    "        large_scaled = scaler.transform(transformed[dof]['DM40'])\n",
    "        \n",
    "        plt.title(f'DOF NUMBER: {dof+1}')\n",
    "        plt.plot(healthy_scaled[0:50,:].T,'g', label = label_healthy, alpha = 0.4);\n",
    "        plt.plot(mod_scaled[0:50,:].T,'b', label = label_moderate, alpha = 0.4);\n",
    "        plt.plot(large_scaled[0:50,:].T,'r', label = label_large, alpha = 0.4);\n",
    "\n",
    "        plt.xlim([0,100])\n",
    "        plt.xlabel('Frequency [Hz]')\n",
    "        plt.ylabel('Magnitude')\n",
    "\n",
    "        leg = plt.legend()\n",
    "        for lh in leg.legendHandles: \n",
    "            lh.set_alpha(1)\n",
    "            \n",
    "        scaled[dof]['DM00'] = healthy_scaled[:,:100]\n",
    "        scaled[dof]['DM20'] = mod_scaled[:,:100]\n",
    "        scaled[dof]['DM40'] = large_scaled[:,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1eb8a",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48a3e4",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd677ec",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4dceb6",
   "metadata": {},
   "source": [
    "# Running UMAP $\\rightarrow$ HDBSCAN Algorithm\n",
    "\n",
    "This is to be run on the *transformed* data as per above. In this code it therefore assumes that all plots are run (because the data is pre-processing in each instance).\n",
    "\n",
    "Note also that this function takes n_healty, n_mod, and n_large as inputs. It must be stressed that *these are not used* in the algorithm -- they are simply there to assist in the colors when plotting. The only input ever being using is that of the *total stacked data*.\n",
    "\n",
    "Looking at the previous scaling, the ```fit-transform``` was perform directly on the healthy, and the ```transform``` on the modeerate (DM20), and then large (DM40) cases. This may initially come across as a data leakage problem, but in fact this is not the case. Because we operate under the assumption that the first X amount of days / weeks / months, the bridge should be operating within a healthy state, and therefore it is OK to perform a fit-transform here. From there-onwards, it is appropriate to continually apply the t```ransform``` only regardless of the health state (because if it is still healthy, the data will map to the same space, and if it contains damage, it will map to a different location). \n",
    "\n",
    "This option can be explored through a hyperparameter in the ```HDBSCAN_UMAP_Application``` function via ```total_obs```. This hyperparameter works to partition the stacked data into distinct observation sets e.g. by default ```total_obs=2```, so we assume that the total data stack is split into two, so the first half will contain DM00 and DM20 data, whilst the second half will contain a mixture of DM20, and DM40 data. \n",
    "\n",
    "In a practical implementation, the scalar transformations *should* be more ideally performed within the ```HDBSCAN_UMAP_Application``` function (transform the data live -- as it is being received) but for the sake of demonstration (i.e. clearly separating out the steps), this has been externalized from the function in the above step.\n",
    "\n",
    "Note that even though there are 400 data in directory, beacuse we are over sampling (using the boot-strap like procedure) the ```n_healthy```, ```n_mod```, ```n_large``` will be the same as the ```sample_times``` chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca465d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_healthy, n_mod, n_large = sample_times, sample_times, sample_times\n",
    "for dof in range(DOF_NUM):\n",
    "    total_data = np.vstack((scaled[dof]['DM00'], scaled[dof]['DM20'], scaled[dof]['DM40']))\n",
    "    methodology.HDBSCAN_UMAP_Application(total_data, n_healthy, n_mod, n_large, dof, plot_flag = True,\n",
    "                                        set_op_mix_ratio=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841282aa",
   "metadata": {},
   "source": [
    "Note that it is also possible to check the results obtained by the proposed procedure against theoretical results. That is, by performing the UMAP projection *only*, and then assigning a color to each row, with the knowledge that *we know* which cluster each row belongs to. This can be achieved using the following code. \n",
    "\n",
    "This time instead however, we shall set the ```total_obs``` value to 5. In other words, our stack of data arrives to us in 5 discrete increments / time observations. \n",
    "\n",
    "Note also that the UMAP embeddings between the ```theoretical_clustering``` and ```HDBSCAN_UMAP_Application``` functions may not always necessarily be to the same locations, since the random seed slightly changes as we move from one function on to the next. What's important however, is that if separability exists in one function, it should also exist in the other function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918c68d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for dof in range(DOF_NUM):\n",
    "    methodology.theoretical_clustering(total_data, n_healthy, n_mod, n_large, dof, plot_flag = True, total_obs = 2,\n",
    "                                      set_op_mix_ratio=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115caaa",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33801ac7",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275fda2",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebc90b",
   "metadata": {},
   "source": [
    "# Conclusion / Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e8867",
   "metadata": {},
   "source": [
    "As can be seen above for the provided case of P00 - V01 - B09 - DL50, the data pipeline successfully spearates out each of the damage cases quite easily. The *exact* same hyper-parameters have been also shown to work on the following data cases:\n",
    "\n",
    " - P00 - V01 - B09 - DL25\n",
    " - PA1 - V01 - B09 - DL50\n",
    " - PA1 - V01 - B09 - DL25\n",
    " - PA2 - V01 - B09 - DL50\n",
    " \n",
    "There is however a slight difficulty when it comes to the case of PA2 - V01 - B09 - DL25 (DOF1), and hyper-parameter tuning will be required on behalf of the user. A quick solution (based on observation) is to \n",
    "\n",
    " 1. Increase ```n_neighbors``` to a value of 60 so as to increase the *diameter* the UMAP scans for clusters, \n",
    " 2. Increase ```min_cluster_size``` to a value of 95 for similar reasons, \n",
    " 3. Decrease ```set_op_mix_ratio``` to a value of 0.90, to slightly enforce further separation between any clusters found in the dimensionality reduction, and to \n",
    " 4. Increase ```sample_times``` to a value of 3000, to try and allow for the clusters to increase in overall density. \n",
    "\n",
    "It will be noted that even then, there is some \"overlap\" over data points so that they can be slightly mis-specified (i.e. the probability of allocation between clusters approaches an even split, so in the event of two clusters this would be a 50%). Any true mis-allocations of points can be shown easily via ```theoretical_clustering``` function. However these points will be *greyed-out* in the ```HDBSCAN_UMAP_Application``` function (moreover outlier points as determined from HDBSCAN will also be labelled). If one wants these not to be shown then one can set the ```remove_outliers``` and ```remove_misspec``` hyperparameters to ```True```. The method to determine these points is currently not optimal and based on determining the *mode*\n",
    "\n",
    "Further, the projections for V01 - B09 were done to two dimensional space, for visual purposes. If the problem increases in complexity, then it may be necessary to increase ```n_components``` to a value greater than two.\n",
    "\n",
    "We hope this work inspires further development "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
