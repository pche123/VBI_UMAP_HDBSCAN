{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "019e9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Test out simple module importation\n",
    "# 1. Re run the cases and makes sure they align with the overleaf\n",
    "# 2. Condense the code into two for loops and remove extraneous sections / libraries\n",
    "# 3. Comment on functions\n",
    "# 4. Move Python files into .py\n",
    "# 5. Test that this works by importing them / using into a new notebook ---> running again a check for the overleaf cases\n",
    "\n",
    "# ---> Make sure to do a github update at each stage here / figure out when you want to create a pull request for main\n",
    "\n",
    "# remaining challeneges\n",
    "\n",
    "# upload to github\n",
    "# label all figures\n",
    "# set seeds\n",
    "# consistent colours with hdbscsn as new data arrives?\n",
    "# for loop over DOFs (vehicles)\n",
    "# record library versions\n",
    "# explicitly make the cut at 60 Hz\n",
    "\n",
    "# remember bootstrap update is in two locations because u dont have for loop\n",
    "\n",
    "# Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbf7952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/prasad/Documents/Mehri_International_Collab/github/VBI_UMAP_HDBSCAN'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b242d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey there this is a test\n"
     ]
    }
   ],
   "source": [
    "print('hey there this is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fce6da4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "from scipy.linalg import svd\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import permutation\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MaxAbsScaler\n",
    "import random \n",
    "import collections \n",
    "from sklearn.svm import OneClassSVM \n",
    "from collections import Counter\n",
    "from scipy import stats as st\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize']=[15, 10]\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "mpl.rcParams['font.size'] = '32'   # 32 before # 22 for the bad case\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7580060",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1bb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/11955000/how-to-preserve-matlab-struct-when-accessing-in-python\n",
    "def _check_keys( dict):\n",
    "    \"\"\"\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    \"\"\"\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], scipy.io.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict\n",
    "\n",
    "\n",
    "def _todict(matobj):\n",
    "    \"\"\"\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    \"\"\"\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, scipy.io.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "\n",
    "def loadmat(filename):\n",
    "    \"\"\"\n",
    "    this function should be called instead of direct scipy.io .loadmat\n",
    "    as it cures the problem of not properly recovering python dictionaries\n",
    "    from mat files. It calls the function check keys to cure all entries\n",
    "    which are still mat-objects\n",
    "    \"\"\"\n",
    "    data = scipy.io.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "    return _check_keys(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda234a",
   "metadata": {},
   "source": [
    "## Utility Functions (Reading in Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6275d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_fft(Bridge_Type, Damage_Location, Vehicle_Type, Bridge_Profile, \n",
    "                  Damage_Levels = ['DM00', 'DM20', 'DM40'], \n",
    "                  NUM_DATA_IN_DIR = 400 , \n",
    "                  DATA_ROOT_LOC = 'data/', \n",
    "                  NFFT=2**7):\n",
    "\n",
    "    \n",
    "    if Vehicle_Type == 'V1':\n",
    "        DOF = 2\n",
    "    elif Vehicle_Type == 'V2':\n",
    "        DOF = 4\n",
    "    else:\n",
    "        DOF = 8\n",
    "        \n",
    "    # List of Dicts \n",
    "    # len(list) = num DOFs\n",
    "    # Each element in list (dict) is the data for one damage level\n",
    "    data_dict_DOFS = []\n",
    "    for _ in range(DOF):\n",
    "        data_dict_DOFS.append({})\n",
    "    \n",
    "    for i in range(len(Damage_Levels)):\n",
    "        str_root_location = Bridge_Type + '/' + Damage_Location + '/' + Damage_Levels[i] + '/' + Vehicle_Type + '/' + Bridge_Profile + '/'\n",
    "        str_root_event_file = Bridge_Type + Damage_Location + Damage_Levels[i] + Vehicle_Type + Bridge_Profile + 'E'\n",
    "        \n",
    "        # List of Lists\n",
    "        # len(list) = num DOFs\n",
    "        # Each element in list (another list) stores the data for one damage level\n",
    "        # This gets reset per damage level (the history of features per damage level)\n",
    "        # Is designed to be stored in the outer dictionary\n",
    "        event_list_DOFS = []\n",
    "        for _ in range(DOF):\n",
    "            event_list_DOFS.append([])\n",
    "        \n",
    "        # For each event\n",
    "        for j in range(1,NUM_DATA_IN_DIR+1): \n",
    "            data_str = DATA_ROOT_LOC + str_root_location + str_root_event_file + f'{j:04d}' + '.mat'\n",
    "            data_read = loadmat(data_str)\n",
    "            \n",
    "            start_pos = data_read['Event']['Veh']['Pos']['t0_ind_beam']\n",
    "            end_pos = data_read['Event']['Veh']['Pos']['t_end_ind_beam']\n",
    "            data_matrix = data_read['Event']['Sol']['Veh']['A'][:,start_pos:end_pos] # important to keep start:end range\n",
    "                        \n",
    "            features = fft_feature_engineering(data_matrix, nfft=NFFT)\n",
    "            # Keep accumulating the features, across event, for each observed DOF, at a specific damage level\n",
    "            for dof_num in range(DOF):\n",
    "                event_list_DOFS[dof_num] += [features[dof_num,:]]\n",
    "\n",
    "            \n",
    "            if j%50 == 0: print(f'Processed Event {j} from damage level {Damage_Levels[i]}')\n",
    "            \n",
    "        for dof_num in range(DOF):\n",
    "            data_dict_DOFS[dof_num][Damage_Levels[i]] = np.array(event_list_DOFS[dof_num] )\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    print( f'Finished processing information from:\\n'\n",
    "           f'Bridge - {Bridge_Type}\\n'\n",
    "           f'Location - {Damage_Location}\\n' \n",
    "           f'Vehcile Type - {Vehicle_Type}\\n'\n",
    "           f'Road Profile - {Bridge_Profile}!\\n\\n')\n",
    "    \n",
    "    return data_dict_DOFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc0e13",
   "metadata": {},
   "source": [
    "# Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b915ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_feature_engineering(data_array, nfft):\n",
    "    '''\n",
    "    Refers to one observed car data point (one row of input file)\n",
    "    data_array.shape = (DOFs, end_pos-start_pos)\n",
    "    '''\n",
    "    # TODO: Consider about the order of the sum then the log -- is there some theoretical comment here?\n",
    "    return fft(data_array - data_array.mean(axis=1).reshape(-1,1), axis=1, n=nfft)[:,:nfft//2]\n",
    "    \n",
    "    \n",
    "def log_abs_transform(data):\n",
    "    return np.log(np.abs(data))\n",
    "\n",
    "\n",
    "def bootstrap_sampling(data_array_ffts, sample_times=1000):\n",
    "    '''\n",
    "    Assumes all the data have been collected into their own post fft [N x nfft] arrays / DOF\n",
    "\n",
    "    idxs <-- randperm(N)\n",
    "    train / test / val split (idxs)\n",
    "\n",
    "    Splits generated once to avoid train-val-test split data leakage problems.\n",
    "    '''\n",
    "    \n",
    "\n",
    "    # Wilcox(2010) writes \"599 is recommended for general use.\"\n",
    "    # Wilcox, R. R. (2010). Fundamentals of modern statistical methods: Substantially improving power and accuracy. Springer.\n",
    "    \n",
    "    num_data, array_length = data_array_ffts.shape # shape of input data\n",
    "    idxs = permutation(num_data) # random permutation over the indices\n",
    "    \n",
    "      \n",
    "    data_bootstrap_samples = []\n",
    "\n",
    "    for _ in range(sample_times):\n",
    "        idxs_resample = resample(idxs)\n",
    "        data_bootstrap_samples.append(np.mean(data_array_ffts[idxs_resample,:], axis=0))\n",
    "\n",
    "    return [np.array(data_bootstrap_samples)]\n",
    "\n",
    "\n",
    "\n",
    "### I need to see which sections have the majority of the [0,1,2] splits\n",
    "# and not just see \"which is the first one to appear\"\n",
    "def remap(clustering_labels):\n",
    "    '''\n",
    "    remap clustering to reflect order of appearance\n",
    "    '''\n",
    "    remaped_indices = []\n",
    "    _, idx_order = np.unique(clustering_labels,return_index=True)\n",
    "    value_order = list(clustering_labels[np.sort(idx_order)])\n",
    "        \n",
    "    print(idx_order)\n",
    "    print(value_order)\n",
    "    if -1 in value_order: value_order.remove(-1)\n",
    "    \n",
    "    print(value_order)\n",
    "    for L in clustering_labels:\n",
    "        if L == -1: \n",
    "            remaped_indices.append(-1)\n",
    "            continue\n",
    "        count = 0\n",
    "        for val in value_order:\n",
    "            if val == L:\n",
    "                remaped_indices.append(count)\n",
    "            count += 1\n",
    "        \n",
    "    return remaped_indices\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc4ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be36ce19",
   "metadata": {},
   "source": [
    "# Methodology Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab438ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs updating of default values\n",
    "def theoretical_clustering(data,\n",
    "                           n_healthy,\n",
    "                           n_mod,\n",
    "                           n_large,\n",
    "                           n_components = 2, \n",
    "                           n_neighbors = 5, \n",
    "                           min_dist = 1, \n",
    "                           metric = 'manhattan', \n",
    "                           set_op_mix_ratio=0.7, \n",
    "                           plot_flag=True,\n",
    "                           print_flag=True,\n",
    "                           total_obs=5):\n",
    "    \n",
    "\n",
    "    '''\n",
    "    The theoretical grouping to expect, based on a supervised knowledge\n",
    "    of which data is healthy, moderate damag, and large damage.\n",
    "    '''\n",
    "    \n",
    "    n_data = data.shape[0]\n",
    "    cs = ['1']*n_healthy + ['2']*n_mod + ['3']*n_large # Theoretical Groupings\n",
    "    umap_results = {}\n",
    "    \n",
    "    for i in range(1,total_obs+1):\n",
    "        sub_data = data[0:n_data*i//total_obs,:]\n",
    "        sub_colours = list(map(int,cs))[0:n_data*i//total_obs] # string to int\n",
    "\n",
    "        umap_obj = umap.UMAP(n_neighbors = n_neighbors, n_components= n_components, \n",
    "                             min_dist=min_dist,metric=metric,\n",
    "                             set_op_mix_ratio=set_op_mix_ratio, random_state=42).fit(sub_data)\n",
    "        \n",
    "        data_points = umap_obj.transform(sub_data)\n",
    "\n",
    "        if plot_flag:\n",
    "            plt.figure()\n",
    "            plt.xlabel('X Magnitude')\n",
    "            plt.ylabel('Y Magnitude')\n",
    "            \n",
    "            DM00_flag = True\n",
    "            DM20_flag = True\n",
    "            DM40_flag = True\n",
    "            \n",
    "            # inefficient looping\n",
    "            for j, col in enumerate(sub_colours):\n",
    "                if col == 1:\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='g',edgecolors='k',s=150,alpha=0.7,\n",
    "                             label = 'DM00' if DM00_flag else None)\n",
    "                    DM00_flag = False\n",
    "                elif col == 2:\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='b',edgecolors='k',s=150,alpha=0.7,\n",
    "                            label = 'DM20' if DM20_flag else None)\n",
    "                    DM20_flag = False\n",
    "                else:\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='r',edgecolors='k',s=150,alpha=0.7,\n",
    "                            label = 'DM40' if DM40_flag else None)\n",
    "                    DM40_flag = False\n",
    "                    \n",
    "            plt.legend()\n",
    "            leg = plt.legend()\n",
    "            for lh in leg.legendHandles: \n",
    "                lh._sizes = [300]\n",
    "\n",
    "                    \n",
    "        if print_flag: print(f'{i}/{total_obs}')\n",
    "        \n",
    "        umap_results[f'{i}_Umap_Object'] = umap_obj\n",
    "        umap_results[f'{i}_Sub_Data_Fit'] = sub_data\n",
    "        umap_results[f'{i}_Color_Labels'] = sub_colours\n",
    "        \n",
    "    return umap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d4da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs updating of default values\n",
    "def HDBSCAN_UMAP_Application(data,\n",
    "                           n_healthy,\n",
    "                           n_mod,\n",
    "                           n_large,\n",
    "                           n_components = 2, \n",
    "                           n_neighbors = 5, \n",
    "                           min_dist = 1, \n",
    "                           metric = 'manhattan', \n",
    "                           set_op_mix_ratio=0.7, \n",
    "                           plot_flag=True,\n",
    "                           print_flag=True,\n",
    "                           total_obs=2,\n",
    "                           min_cluster_size=60,\n",
    "                           remove_outliers=False,\n",
    "                           remove_misspec=False):\n",
    "    \n",
    "    '''\n",
    "    n_healthy, n_mod, and n_large are ONLY used to assist in plotting. The entire \"data\" is used in the \n",
    "    UMAP and HDBSCAN procedure without knowing the boundaries of healthy->mod->large dam transitions \n",
    "    (unlike in the \"theoretical_clustering\" function.)\n",
    "    ''' \n",
    "\n",
    "    n_data = data.shape[0]\n",
    "\n",
    "    for i in range(1,total_obs+1):\n",
    "        sub_data = total_data[0:n_data*i//total_obs,:]\n",
    "\n",
    "        # Use Manhattan --> higher dim\n",
    "        umap_obj = umap.UMAP(n_neighbors = n_neighbors, n_components = n_components, \n",
    "                             min_dist = min_dist,metric='manhattan',\n",
    "                             set_op_mix_ratio = set_op_mix_ratio, random_state=42).fit(sub_data)\n",
    "\n",
    "        data_points = umap_obj.transform(sub_data)\n",
    "\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(metric='euclidean', \n",
    "                                    prediction_data=True, \n",
    "                                    min_cluster_size = min_cluster_size).fit(data_points)\n",
    "\n",
    "\n",
    "        if plot_flag:\n",
    "            plt.figure()\n",
    "            plt.xlabel('X Magnitude')\n",
    "            plt.ylabel('Y Magnitude')\n",
    "            \n",
    "            DM00_flag = True\n",
    "            DM20_flag = True\n",
    "            DM40_flag = True\n",
    "            Outlier_flag = True\n",
    "            Misspeficiation_flag = True\n",
    "            \n",
    "            healthy_label = None\n",
    "            mod_label = None\n",
    "            large_label = None\n",
    "            outlier_label = -1\n",
    "                \n",
    "            if len(clusterer.labels_) <= n_healthy:\n",
    "                 healthy_label = int(st.mode(clusterer.labels_)[0])\n",
    "                    \n",
    "            elif len(clusterer.labels_) <= n_healthy + n_mod:  \n",
    "                healthy_label = int(st.mode(clusterer.labels_[0:n_healthy])[0])\n",
    "                mod_label = int(st.mode(clusterer.labels_[n_healthy:-1])[0])\n",
    "                \n",
    "            else:\n",
    "                healthy_label = int(st.mode(clusterer.labels_[0:n_healthy])[0])\n",
    "                mod_label = int(st.mode(clusterer.labels_[n_healthy:n_healthy+n_mod])[0])\n",
    "                large_label = int(st.mode(clusterer.labels_[n_healthy+n_mod:-1])[0])\n",
    "\n",
    "            for j,label in enumerate(clusterer.labels_):\n",
    "                if label == healthy_label:\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='g',edgecolors='k',s=150,alpha=0.7,\n",
    "                             label = 'DM00' if DM00_flag else None)\n",
    "                    DM00_flag = False\n",
    "                elif label == mod_label:\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='b',edgecolors='k',s=150,alpha=0.7,\n",
    "                                label = 'DM20' if DM20_flag else None)\n",
    "                    DM20_flag = False  \n",
    "\n",
    "                elif label == large_label:\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='r',edgecolors='k',s=150,alpha=0.7,\n",
    "                            label = 'DM40' if DM40_flag else None)\n",
    "                    DM40_flag = False\n",
    "                    \n",
    "                elif label == outlier_label:\n",
    "                    if remove_outliers == True:\n",
    "                        continue\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='k',edgecolors='k',s=150,alpha=0.3,\n",
    "                            label = 'Outlier' if Outlier_flag else None)\n",
    "                    Outlier_flag = False    \n",
    "                    \n",
    "                else:\n",
    "                    if remove_misspec == True:\n",
    "                        continue\n",
    "                    plt.scatter(data_points[j,0],data_points[j,1],c='y',edgecolors='k',s=150,alpha=0.3,\n",
    "                            label = 'Misspecified Point(s)' if Misspeficiation_flag else None)\n",
    "                    Misspeficiation_flag = False       \n",
    "                    \n",
    "            plt.legend()\n",
    "            leg = plt.legend()\n",
    "            for lh in leg.legendHandles: \n",
    "                lh._sizes = [300]\n",
    "\n",
    "            \n",
    "        if print_flag:\n",
    "            print(f'{i}/{total_obs}')\n",
    "            \n",
    "    # Final Clustering and Label Allocations\n",
    "    return [hdbscan.all_points_membership_vectors(clusterer), clusterer.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee523c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "244e7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a166b6ed",
   "metadata": {},
   "source": [
    "# Code Starts Here: Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2bf6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFFT = 2**7\n",
    "DATA_ROOT_LOC = 'data/'\n",
    "NUM_DATA_IN_DIR = 400\n",
    "\n",
    "Bridge_Type = 'B09'                          # B09 | B15 | B21 | B27 | B33 | B39\n",
    "Damage_Location = 'DL50'                     # DL25 | DL50\n",
    "Damage_Levels = ['DM00', 'DM20', 'DM40']     # DM00 (\"Healthy\") | DM20 | DM40\n",
    "Vehicle_Type = 'V1'                          # V1 | V2 | V5\n",
    "Bridge_Profile = 'P00'                       # P00 | PA1 | PA2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Not needed right now?\n",
    "# FS = 512 # Sampling rate [Hz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d372ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/B09/DL50/DM00/V1/P00/B09DL50DM00V1P00E0001.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/B09/DL50/DM00/V1/P00/B09DL50DM00V1P00E0001.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_data_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBridge_Type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDamage_Location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVehicle_Type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBridge_Profile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mNUM_DATA_IN_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_DATA_IN_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mDATA_ROOT_LOC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDATA_ROOT_LOC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mNFFT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNFFT\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mread_data_fft\u001b[0;34m(Bridge_Type, Damage_Location, Vehicle_Type, Bridge_Profile, Damage_Levels, NUM_DATA_IN_DIR, DATA_ROOT_LOC, NFFT)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,NUM_DATA_IN_DIR\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m): \n\u001b[1;32m     37\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m DATA_ROOT_LOC \u001b[38;5;241m+\u001b[39m str_root_location \u001b[38;5;241m+\u001b[39m str_root_event_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 38\u001b[0m     data_read \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     start_pos \u001b[38;5;241m=\u001b[39m data_read[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVeh\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt0_ind_beam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     41\u001b[0m     end_pos \u001b[38;5;241m=\u001b[39m data_read[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVeh\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPos\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_end_ind_beam\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadmat\u001b[39m(filename):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    this function should be called instead of direct scipy.io .loadmat\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    as it cures the problem of not properly recovering python dictionaries\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    from mat files. It calls the function check keys to cure all entries\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    which are still mat-objects\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct_as_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msqueeze_me\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check_keys(data)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:224\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    225\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/scipy/io/matlab/mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/B09/DL50/DM00/V1/P00/B09DL50DM00V1P00E0001.mat'"
     ]
    }
   ],
   "source": [
    "data = read_data_fft(Bridge_Type, Damage_Location, Vehicle_Type, Bridge_Profile,\n",
    "                    NUM_DATA_IN_DIR = NUM_DATA_IN_DIR , \n",
    "                    DATA_ROOT_LOC = DATA_ROOT_LOC, \n",
    "                    NFFT=NFFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735691a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7c743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350813e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371bed55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0232b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c64b8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67993e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3b828d8",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f634549",
   "metadata": {},
   "source": [
    "### UMAP PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "n_neighbors = 30 # 30 is good\n",
    "min_dist = 0\n",
    "set_op_mix_ratio = 0.95\n",
    "\n",
    "total_obs = 2\n",
    "\n",
    "min_cluster_size = 60 # make sure to update the functinos default values!!\n",
    "\n",
    "sample_times = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935468b9",
   "metadata": {},
   "source": [
    "### HDBSCAN PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2116cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b9a5d",
   "metadata": {},
   "source": [
    "## Bootstrap PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e141d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840bef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7622b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL25 / V1 / PA1\n",
    "\n",
    "# n_components = 2\n",
    "# n_neighbors = 60 # 30 is good\n",
    "# min_dist = 0\n",
    "# set_op_mix_ratio = 0.90\n",
    "\n",
    "# total_obs = 2\n",
    "\n",
    "# min_cluster_size = 95\n",
    "\n",
    "# sample_times = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea8996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11a09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ef83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82d6c803",
   "metadata": {},
   "source": [
    "### Analysis - Demo of data pure time domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37514c2b",
   "metadata": {},
   "source": [
    "### Analysis - Demo of data without Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOF_NUM = 1\n",
    "\n",
    "DOF_NUM -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842057f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_healthy = ['DM00'] \n",
    "label_moderate = ['DM20']\n",
    "label_large = ['DM40']\n",
    "for i in range(1,50):\n",
    "    label_healthy += [None]\n",
    "    label_moderate += [None]\n",
    "    label_large += [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9136b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [:,1:] indexing due to infinity at first term\n",
    "\n",
    "# DEMO - NO BOOTSTRAP SAMPLING\n",
    "\n",
    "\n",
    "# FFT only\n",
    "plt.plot(abs(data[DOF_NUM]['DM00'][0:50,1:].T),'g', alpha = 0.4, label = label_healthy);\n",
    "plt.plot(abs(data[DOF_NUM]['DM20'][0:50,1:].T),'b', alpha = 0.4, label = label_moderate);\n",
    "plt.plot(abs(data[DOF_NUM]['DM40'][0:50,1:].T),'r', alpha = 0.4, label = label_large);\n",
    "cols = abs(data[DOF_NUM]['DM00'][:,1:].T).shape[1]\n",
    "# plt.xlim([0,cols])\n",
    "\n",
    "\n",
    "plt.xlim([0,60])\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude')\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab90c21",
   "metadata": {},
   "source": [
    "### Analysis - Data with Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_data_fft = data[DOF_NUM]['DM00']\n",
    "mod_data_fft = data[DOF_NUM]['DM20']\n",
    "large_data_fft = data[DOF_NUM]['DM40']\n",
    "\n",
    "# Perform bootstrapping\n",
    "### TESTING INCREASE IN SAMPLE_TIMES\n",
    "healthy_bootstrap = bootstrap_sampling(healthy_data_fft, sample_times=sample_times) \n",
    "mod_bootstrap = bootstrap_sampling(mod_data_fft, sample_times=sample_times) \n",
    "large_bootstrap = bootstrap_sampling(large_data_fft, sample_times=sample_times) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715d8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_bootstrap[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b44517",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_log_abs_train = log_abs_transform(healthy_bootstrap[0])\n",
    "mod_log_abs_train = log_abs_transform(mod_bootstrap[0])\n",
    "large_log_abs_train = log_abs_transform(large_bootstrap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432460a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.plot(data[DOF_NUM]['DM00'][1:200,:].T,'g');  # STILL IN COMPLEX FORM\n",
    "plt.plot(healthy_log_abs_train[0:50,:].T,'g', label = label_healthy, alpha = 0.4);\n",
    "plt.plot(mod_log_abs_train[0:50,:].T,'b', label = label_moderate, alpha = 0.4);\n",
    "plt.plot(large_log_abs_train[0:50,:].T,'r', label = label_large, alpha = 0.4);\n",
    "plt.xlim([0,60])\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbd32fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167ebfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "healthy_scaled = scaler.fit_transform(healthy_log_abs_train) #### You NEED TO MAKE SURE THIS SPLIT IS CONSISTENT E.G. WHEN INPUTING THE DATASTREAMS SEQUENCING\n",
    "mod_scaled = scaler.transform(mod_log_abs_train)\n",
    "large_scaled = scaler.transform(large_log_abs_train)\n",
    "\n",
    "\n",
    "plt.plot(healthy_scaled[0:50,:].T,'g', label = label_healthy, alpha = 0.4);\n",
    "plt.plot(mod_scaled[0:50,:].T,'b', label = label_moderate, alpha = 0.4);\n",
    "plt.plot(large_scaled[0:50,:].T,'r', label = label_large, alpha = 0.4);\n",
    "\n",
    "plt.xlim([0,60])\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ac976",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_data = np.vstack((healthy_scaled[:,1:], mod_scaled[:,1:], large_scaled[:,1:]))\n",
    "\n",
    "n_healthy = healthy_scaled[:,1:].shape[0]\n",
    "n_mod = mod_scaled[:,1:].shape[0]\n",
    "n_large = large_scaled[:,1:].shape[0]\n",
    "\n",
    "\n",
    "umap_theoretical_results = theoretical_clustering(total_data, n_healthy, n_mod, n_large, \n",
    "                                                  plot_flag=True, print_flag=True,\n",
    "                                                  n_components = n_components, \n",
    "                                                  n_neighbors = n_neighbors,\n",
    "                                                  min_dist = min_dist,\n",
    "                                                  set_op_mix_ratio = set_op_mix_ratio,\n",
    "                                                  total_obs=total_obs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73c979",
   "metadata": {},
   "source": [
    "# HDBSCAN Implementation\n",
    "\n",
    "Quoting from the paper, “On the Surprising Behavior of Distance Metrics in High Dimensional Space”, by Charu C. Aggarwal, Alexander Hinneburg, and Daniel A. Kiem. “ for a given problem with a fixed (high) value of the dimensionality d, it may be preferable to use lower values of p. This means that the L1 distance metric (Manhattan Distance metric) is the most preferable for high dimensional applications.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98333e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_cluster_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dec42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add one to remove \"mis-specified points also if wanted\"\n",
    "output_1 = HDBSCAN_UMAP_Application(total_data, n_healthy, n_mod, n_large,\n",
    "                              plot_flag=True, print_flag=True,\n",
    "                                n_components = n_components, \n",
    "                                n_neighbors = n_neighbors,\n",
    "                                min_dist = min_dist,\n",
    "                                set_op_mix_ratio = set_op_mix_ratio,\n",
    "                                total_obs=1, ############## change back\n",
    "                                min_cluster_size=min_cluster_size,\n",
    "                                remove_outliers=False,  # For the difficult case\n",
    "                                remove_misspec=False); #######################\n",
    "\n",
    "soft_clusters_1, labels_1 = output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bdcc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2789834c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9bd855",
   "metadata": {},
   "source": [
    "## Running Same Thing for Second DoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOF_NUM = 2\n",
    "\n",
    "DOF_NUM -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121162d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FFT only\n",
    "plt.figure()\n",
    "plt.plot(abs(data[DOF_NUM]['DM00'][:50,1:].T),'g', alpha = 0.1, label = label_healthy);\n",
    "plt.plot(abs(data[DOF_NUM]['DM20'][:50,1:].T),'b', alpha = 0.1, label = label_moderate);\n",
    "plt.plot(abs(data[DOF_NUM]['DM40'][:50,1:].T),'r', alpha = 0.1, label = label_large);\n",
    "cols = abs(data[DOF_NUM]['DM00'][:,1:].T).shape[1]\n",
    "\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "\n",
    "\n",
    "plt.xlim([0,60])\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "healthy_data_fft = data[DOF_NUM]['DM00']\n",
    "mod_data_fft = data[DOF_NUM]['DM20']\n",
    "large_data_fft = data[DOF_NUM]['DM40']\n",
    "\n",
    "# Perform bootstrapping\n",
    "### TESTING INCREASE IN SAMPLE_TIMES\n",
    "healthy_bootstrap = bootstrap_sampling(healthy_data_fft, sample_times=sample_times) \n",
    "mod_bootstrap = bootstrap_sampling(mod_data_fft,  sample_times=sample_times) \n",
    "large_bootstrap = bootstrap_sampling(large_data_fft,  sample_times=sample_times) \n",
    "\n",
    "\n",
    "healthy_log_abs_train = log_abs_transform(healthy_bootstrap[0])\n",
    "mod_log_abs_train = log_abs_transform(mod_bootstrap[0])\n",
    "large_log_abs_train = log_abs_transform(large_bootstrap[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(healthy_log_abs_train[0:50,:].T, label = label_healthy, color='g');\n",
    "plt.plot(mod_log_abs_train[0:50,:].T, label = label_moderate, color='b');\n",
    "plt.plot(large_log_abs_train[0:50,:].T, label = label_large, color='r');\n",
    "plt.xlim([0,60])\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "healthy_scaled = scaler.fit_transform(healthy_log_abs_train)\n",
    "mod_scaled = scaler.transform(mod_log_abs_train)\n",
    "large_scaled = scaler.transform(large_log_abs_train)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(healthy_scaled[0:50,:].T,'g');\n",
    "plt.plot(mod_scaled[0:50,:].T,'b');\n",
    "plt.plot(large_scaled[0:50,:].T,'r');\n",
    "\n",
    "plt.xlim([0,60])\n",
    "plt.xlabel('Frequency [Hz]')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles: \n",
    "    lh.set_alpha(1)\n",
    "\n",
    "\n",
    "total_data = np.vstack((healthy_scaled[:,1:], mod_scaled[:,1:], large_scaled[:,1:]))\n",
    "\n",
    "n_healthy = healthy_scaled[:,1:].shape[0]\n",
    "n_mod = mod_scaled[:,1:].shape[0]\n",
    "n_large = large_scaled[:,1:].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac690aa5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "umap_theoretical_results = theoretical_clustering(total_data, n_healthy, n_mod, n_large, \n",
    "                                                  plot_flag=True, print_flag=False,\n",
    "                                                  n_components = n_components, \n",
    "                                                  n_neighbors = n_neighbors,\n",
    "                                                  min_dist = min_dist,\n",
    "                                                  set_op_mix_ratio = set_op_mix_ratio,\n",
    "                                                  total_obs=total_obs);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f81faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2 = HDBSCAN_UMAP_Application(total_data, n_healthy, n_mod, n_large, \n",
    "                              plot_flag=True, print_flag=False,\n",
    "                                n_components = n_components, \n",
    "                                n_neighbors = n_neighbors,\n",
    "                                min_dist = min_dist,\n",
    "                                set_op_mix_ratio = set_op_mix_ratio,\n",
    "                                total_obs=total_obs, \n",
    "                                min_cluster_size=min_cluster_size);\n",
    "\n",
    "soft_clusters_2, labels_2 = output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf62627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d1f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d4e523e",
   "metadata": {},
   "source": [
    "## Testing out Scoring Functions amongst Multiple Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array([[0.6,0.4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c52de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = np.array([[0.9,0.2,0.2,0.2,0.2,0.2]])\n",
    "entropy(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afc462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(arr):\n",
    "    '''\n",
    "    Assumes 2D array input\n",
    "    '''\n",
    "    return -(np.log(arr)*arr).sum(axis=1)\n",
    "\n",
    "def diff(arr):\n",
    "    return arr[1:] - arr[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a504f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ent = entropy(np.ones((1,soft_clusters_1.shape[1]))/soft_clusters_1.shape[1]) + \\\n",
    "          entropy(np.ones((1,soft_clusters_2.shape[1]))/soft_clusters_2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa4fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_1 = diff(labels_1)\n",
    "f_1 = entropy(soft_clusters_1)\n",
    "\n",
    "test_arr_1 = np.zeros(len(changes_1))\n",
    "for i in range(len(changes_1)):\n",
    "    if changes_1[i] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        test_arr_1[i] = f_1[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040278f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_2 = diff(labels_2)\n",
    "f_2 = entropy(soft_clusters_2)\n",
    "\n",
    "test_arr_2 = np.zeros(len(changes_2))\n",
    "for i in range(len(changes_2)):\n",
    "    if changes_2[i] == 0:\n",
    "        continue\n",
    "    else:\n",
    "        test_arr_2[i] = f_2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf64988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Then we can combine entropy scores across each DOF\n",
    "# Should I consider appeals to softmax / cross-entropy arguments?\n",
    "# Is there a principled way to combine entropy scores from independednt generating sources?\n",
    "# https://stats.stackexchange.com/a/100922/117574\n",
    "# https://en.wikipedia.org/wiki/Joint_entropy#Properties\n",
    "\n",
    "# plt.title('Generation of New Cluster Detected')\n",
    "# plt.plot(changes_1) # Has a new cluster formed?\n",
    "# plt.figure()\n",
    "# plt.title('Entropy Score of the New Cluster')\n",
    "# plt.plot(test_arr_1) # What is the quality of the clustering (entropy score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e56332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Generation of New Cluster Detected')\n",
    "# plt.plot(changes_2) # Has a new cluster formed?\n",
    "# plt.figure()\n",
    "# plt.title('Entropy Score of the New Cluster')\n",
    "# plt.plot(test_arr_2) # What is the quality of the clustering (entropy score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d98e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Total (final) Entropy Scoring')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(test_arr_1+test_arr_2,linewidth=5,label='Entropy Value for each Data Point') # What is the quality of the clustering (entropy score)\n",
    "\n",
    "# SHOULD BE TUNED TO BOOTSTRAP LENGTH\n",
    "xlim_length = soft_clusters_1.shape[0]\n",
    "plt.hlines(max_ent,0,xlim_length,'r',linewidth=5, label='Maximum Entropy Value')\n",
    "plt.xlim(0,xlim_length)\n",
    "plt.ylim(0,max_ent+0.7)\n",
    "\n",
    "plt.xlabel('Data Point Number')\n",
    "plt.ylabel('Entropy Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.vlines(xlim_length//3, 0,max_ent+0.7, color='k', linestyle='dashed', linewidth=3)\n",
    "plt.vlines(xlim_length//3*2, 0,max_ent+0.7, color='k', linestyle='dashed', linewidth=3)\n",
    "\n",
    "rect_DM00 = Rectangle((0.0,0.0),xlim_length//3,max_ent+0.7,color='green',alpha=0.2)\n",
    "rect_DM20 = Rectangle((xlim_length//3,0.0),xlim_length//3,max_ent+0.7,color='blue',alpha=0.15)\n",
    "rect_DM40 = Rectangle((xlim_length//3*2,0.0),xlim_length//3,max_ent+0.7,color='red',alpha=0.4)\n",
    "\n",
    "ax.add_patch(rect_DM00)\n",
    "ax.add_patch(rect_DM20)\n",
    "ax.add_patch(rect_DM40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337df279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf679306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
